#there's also python universe. One can check later.
universe = vanilla

#executable is the file that runs
executable = condor_exec.sh

#arguments of the functions in order in the executable
#arguments = ./configFile_LNGS.txt 1 20 12175 /jupyter-workspace/cloud-storage/cygno-data/LNGS/
#arguments = ./configFile_LNGS.txt 1 20 12175 /jupyter-workspace/cloud-storage/cygno-data/LNGS/run12175.mid.gz
arguments = ./configFile_LNGS.txt 4 100 12000

#output logs. Can change the path
error = ./output_files/digi.error
output = ./output_files/digi.out
log = ./output_files/digi.log

#check manual. All paths are relative to this one. easier to work
#initialdir = /jupyter-workspace/private/

#set enviroment variables as the same from where I launched the job
getenv = True

#transfer files in and out
should_transfer_files = yes
when_to_transfer_output = ON_EXIT

#condor needs all the files required to run the job. Create a small folder with only the required files
# for the reco is cleanup of the folder is needed
transfer_input_files = /jupyter-workspace/private/pmt_reco/
#transfer_input_files = /jupyter-workspace/private/pmt_reco/, /jupyter-workspace/cloud-storage/cygno-data/LNGS/run12175.mid.gz

##choose what to transfer. Can transfer an whole folder of outputs.
## Best way is to create an output folder, put whatever results we want there, and then download that folder
## An 'output_files' is created before the pmt_reco goes through
transfer_output_files = ./output_files

##look this in the manual. Not sure how it works
#transfer_output_remaps = "/jupyter-workspace/shared/dmarques/test_shared_folder/"
#output_destination = /jupyter-workspace/shared/dmarques/test_shared_folder/

+OWNER = "condor"

#can queue in a loop with specific variables
#queue 10 executes this 10 times (output would be overwritten)

## to submit
## condor_submit -spool <submitfile>
## condor_transfer_data <jobID>


queue
